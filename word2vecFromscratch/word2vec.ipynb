{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43495c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec from scratch (skip-gram)\tNegative sampling, embeddings\tWikipedia dump or custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370bd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "import random\n",
    "import pickle\n",
    "from typing import List, Tuple,Dict,Set\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9860388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self,vector_size=100,window_size=5,negative_samples=5,learning_rate=0.01,min_count=1,epochs=5):\n",
    "\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.min_count = min_count\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.window=window_size\n",
    "\n",
    "        # Vocabulary and mapping\n",
    "\n",
    "        self.vocab = {}\n",
    "        self. index2word = {}\n",
    "        self.word_counts=Counter()\n",
    "        self.vocab_size=0\n",
    "\n",
    "        #model parameters\n",
    "        self.W1 = None  # Input to hidden layer weights  \n",
    "        self.W2 = None  # Hidden to output layer weights\n",
    "\n",
    "        #for negative sampling\n",
    "        self.unigram_table =None\n",
    "        self.table_size=1e8\n",
    "\n",
    "    def preprocess_text(self,text:str)->List[str]:\n",
    "        text=re.sub(r'[^a-zA-Z\\s]',\"\",text.lower())\n",
    "        #split into words\n",
    "        words=text.split()\n",
    "        return words\n",
    "    def build_vocabulary(self,sentences:List[List[str]]):\n",
    "        print(\"Building vocabulary\")\n",
    "        #count word frequencies\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.word_counts[word]+=1\n",
    "        #filter words below min_count\n",
    "        filtered_words={word:count for word,count in self.word_count.items() if count>=self.min_count}\n",
    "        #create vocabulary mappings\n",
    "        self.vocab={word: i for i ,word in enumerate(filtered_words.keys())}\n",
    "        self.index2word={i:word for word,i in self.vocab.items()}\n",
    "        self.vocab_size=len(self.vocab)\n",
    "\n",
    "        print(f\"Vocabulary size:{self.vocab_size}\")\n",
    "        #build uniform table for negative sampling\n",
    "        self._build_unigram_table()\n",
    "\n",
    "    def _build_unigram_table(self):\n",
    "        print(\"Building unigram table for negative sampling\")\n",
    "        #calculate probabilities\n",
    "        total_count=sum(self.word_counts[word] for word in self.vocab.keys())\n",
    "        word_probs={}\n",
    "        for word in self.vocab.keys():\n",
    "            word_probs[word]=(self.word_counts[word]/total_count)**0.75\n",
    "\n",
    "        #Normalize Probabilities\n",
    "        total_probs=sum(word_probs.values())\n",
    "        for word in word_probs:\n",
    "            word_probs[word]!=total_probs\n",
    "        #create unigram table\n",
    "        self.unigram_table=[]\n",
    "        table_size=int(self.table_size)\n",
    "\n",
    "        for word,prob in word_probs.items():\n",
    "            count=int(prob*table_size)\n",
    "            self.unigram_table.extend([self.vocab[word]]*count)\n",
    "        self.unigram_table=np.array(self.unigram_table)\n",
    "        print(f\"Unigram table size:{len(self.unigram_table)}\")\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        #initialize input embeddings(small random values)\n",
    "        self.w1=np.random.uniform(-0.5,0.5,(self.vocab_size,self.vector_size))\n",
    "        self.w1=self.w1/self.vector_size\n",
    "        #initialize output embeddings(Zeros)\n",
    "        self.w2=np.zeros((self.vocab_size,self.vector_size))\n",
    "    def generate_training_data(self,sentences:List[List[str]]):\n",
    "        #generate skip gram training pairs\n",
    "        training_data=[]\n",
    "        for sentence in sentences:\n",
    "            word_indices=[self.vocab[word] for word in sentence if word in self.vocab]\n",
    "            for i,target_word in enumerate (word_indices):\n",
    "                start=max(0,i-self.window)\n",
    "                end=min(len(word_indices),i+self.window+1)\n",
    "                for j in range (start,end):\n",
    "                    if i!=j:\n",
    "                        context_word=word_indices[j]\n",
    "                        training_data.append((target_word,context_word))\n",
    "        return training_data\n",
    "\n",
    "    def get_negative_samples(self,target_word:int,content_word:int)->List[int]:\n",
    "        negatives=[]\n",
    "        while len(negatives)<self.negatives_samples:\n",
    "            neg_sample=np.random.choice(self.unigram_table)\n",
    "            if neg_sample!=target_word and neg_sample!=content_word:\n",
    "                negatives.append(neg_sample)\n",
    "        return negatives\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        x=np.clip(x,-500,500)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def train_pair(self,target_word:int,context_word:int):\n",
    "        #get embeddingd\n",
    "        target_embed=self.w1[target_word]\n",
    "        #positive sample\n",
    "        context_embed=self.w2[context_word]\n",
    "        #calculate positive sample loss and gradients\n",
    "        pos_score=np.dot(target_embed,context_embed)\n",
    "        pos_prob=self.sigmoid(pos_score)\n",
    "        pos_error=pos_prob-1#target is 1 for positive samples\n",
    "        #update gradients for positive samples\n",
    "        grad_target=pos_error*context_embed\n",
    "        grad_context=pos_error*target_embed\n",
    "        #Apply gradients\n",
    "        self.w1[target_word]-=self.learning_rate*grad_target\n",
    "        self.w2[context_word]-=self.learning_rate*grad_context\n",
    "        #Negative samples\n",
    "        negative_samples=self.get_negative_samples(target_word,context_word)\n",
    "\n",
    "        for neg_word in negative_samples:\n",
    "            neg_embed=self.w2[neg_word]\n",
    "            #Calculate negative sample loss and gradients\n",
    "            neg_score=np.dot(target_embed,neg_embed)\n",
    "            neg_prob=self.sigmoid(neg_score)\n",
    "            neg_error=neg_prob-0#target is 0 for negative samples\n",
    "            #update gradients for negative sample\n",
    "            grad_target+=neg_error*neg_embed\n",
    "            grad_neg=neg_error*target_embed\n",
    "            #Apply gradients\n",
    "            self.w2[neg_word]-=self.learning_rate*grad_neg\n",
    "        #Update target embedding with acccumulated gradients\n",
    "        self.w1[target_word]-=self.learning_rate*grad_target\n",
    "    \n",
    "    def train(self,sentences:List[List[str]]):\n",
    "        print(\"Starting Training....\")\n",
    "        #build vocabulary\n",
    "        self.build_vocabulary(sentences)\n",
    "        #Initialize weights\n",
    "        self.initialize_weights()\n",
    "        #generate Trainng data\n",
    "        training_data=self.generate_training_data(sentences)\n",
    "        print(f\"Generated {len(training_data)} training pairs\")\n",
    "        #Trainning Loop\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n Epoch{epoch+1/self.epochs}\")\n",
    "            #shuffle training data\n",
    "            random.shuffle(training_data)\n",
    "            #Train on each pair\n",
    "            total_loss=0\n",
    "            for target_word,context_word in tqdm(training_data,desc=f\"Epoch{epoch+1}\"):\n",
    "                self.train_pair(target_word,context_word)\n",
    "            #decay learning rate\n",
    "            self.learning_rate*=0.95\n",
    "            print(f\"Learning rate:{self.learning_rate}\")\n",
    "    def get_word_vector(self,word:str)->np.ndarray:\n",
    "        if word in self.vocab:\n",
    "            return self.w1[self.vocab[word]]\n",
    "        else:\n",
    "            raise KeyError(f\"{word} not in vocabulary\") \n",
    "    def most_similar(self,word:str,topn=10)->List[Tuple[str,float]]:\n",
    "        if word not in self.vocab:\n",
    "            raise KeyError(f\"{word} not in vocabulary\")\n",
    "        word_vector=self.get_word_vector(word)\n",
    "        #calculate cosine similarities\n",
    "        similarities=[]\n",
    "        for other_word in self.vocab:\n",
    "            if other_word!=word:\n",
    "                other_vector=self.get_word_vector(other_word)\n",
    "                #cosine similarity\n",
    "                cos_sim=np.dot(word_vector,other_vector)/(np.linalg.norm(word_vector)*np.Linalg.norm(other_vector))\n",
    "                similarities.append((other_word,cos_sim))\n",
    "        #sort by similarity\n",
    "        similarities.sort(key=lambda x:x[1],reverse=True)\n",
    "        return similarities[:topn]\n",
    "    \n",
    "    def save_model(self,filepath:str):\n",
    "        model_data={\n",
    "            'w1':self.w1,\n",
    "            'w2':self.w2,\n",
    "            'vocab':self.vocab,\n",
    "            'index2word':self.index2word,\n",
    "            'word_counts':self.word_counts,\n",
    "            'vector_size':self.vector_size,\n",
    "            'vocab_size':self.vocab_size\n",
    "            }\n",
    "        with open(filepath,'wb') as f:\n",
    "            pickle.dump(model_data,f)\n",
    "        print(f\"Model saved to {filepath}\")      \n",
    "    \n",
    "    def load_model(self,filepath:str):\n",
    "        with open(filepath,\"rb\") as f:\n",
    "            model_data=pickle.load(f)\n",
    "        self.w1=model_data['w1']\n",
    "        self.w2=model_data['w2']\n",
    "        self.vocab=model_data['vocab']\n",
    "        self.index2word=model_data['index2word']\n",
    "        self.word_counts=model_data['word_counts']\n",
    "        self.vector_size=model_data['vector_size']\n",
    "        self.vocab_size=model_data['vocab_size']\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd29a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage and testing\n",
    "def create_sample_corpus():\n",
    "    corpus=[\n",
    "        \"Monika Chaulagain is filthy rich\"\n",
    "        \"Monika Chaulagain is very beautiful\"\n",
    "        \"Monuu is very intelligent.\"\n",
    "        \"Everybody loves Monuu.\"\n",
    "        \"Monika is filled with love ,grace and happieness.\"\n",
    "        \"She has done some great jobss which will be remembered in the manknind history\"\n",
    "        \"Monika Chaulagain is an amazing person\"\n",
    "        \"Monika Chaulagain is a great friend\"\n",
    "    ]*100 #replicate to increase corpus size\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9122d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec skip gram implementation\n",
      "========================================\n",
      "Creating sample corpus...\n",
      "Default initialization works!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Word2Vec.__init__() got an unexpected keyword argument 'window'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDefault initialization works!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Initialize model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model=\u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.025\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#Preprocess corpus\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocessing corpus...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Word2Vec.__init__() got an unexpected keyword argument 'window'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Example of Word2vec implementatiion\n",
    "    print(\"Word2vec skip gram implementation\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(\"Creating sample corpus...\")\n",
    "    raw_corpus=create_sample_corpus()\n",
    "\n",
    "    model = Word2Vec()  # Use all defaults first\n",
    "    print(\"Default initialization works!\")\n",
    "\n",
    "    #Initialize model\n",
    "    model=Word2Vec(\n",
    "        vector_size=50,\n",
    "        window=3,\n",
    "        negative_samples=5,\n",
    "        learning_rate=0.025,\n",
    "        min_count=2,\n",
    "        epochs=10\n",
    "    )\n",
    "    #Preprocess corpus\n",
    "    print(\"Preprocessing corpus...\")\n",
    "    sentences=[model.preprocess_text(text) for text in raw_corpus]\n",
    "    #Train model\n",
    "    model.train(sentences)\n",
    "    #Test model\n",
    "    print(\"\\n\"+\"=\"*40)\n",
    "    print(\"Testing the model:\")\n",
    "    print(\"=\"*40)\n",
    "    test_words=[\"monika\",\"beautiful\",\"intelligent\",\"friend\"]\n",
    "    for word in test_words:\n",
    "        if word in model.vocab:\n",
    "            print(f\"\\nMOst similar to '{word}':\")\n",
    "            similar=model.most_similar(word,topn=5)\n",
    "            for sim_word,sim_score in similar:\n",
    "                print(f\"{sim_word}:{sim_score:.4f}\")\n",
    "    #save model\n",
    "    print(\"\\nSaving model...\")\n",
    "    model.save_model(\"word2vec_model.pkl\")\n",
    "    print(\"\\n Training completed!\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
